(tensorflow_p27) ubuntu@ip-172-31-33-80:~/11695/starter_code$ ./scripts/conv.sh
/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
  /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #2
    (fname, cnt))
    /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #3
      (fname, cnt))
      --------------------------------------------------------------------------------
      Path outputs exists. Remove and remake.
      --------------------------------------------------------------------------------
      data_path.................................................../cifar-10-batches-py
      log_every....................................................................100
      model_name..................................................................conv
      output_dir...............................................................outputs
      reset_output_dir............................................................True
      --------------------------------------------------------------------------------
      Reading data
      data_batch_1
      data_batch_2
      data_batch_3
      data_batch_4
      data_batch_5
      test_batch
      Prepropcess: [subtract mean], [divide std]
      mean: [125.34512 122.94169 113.83898]
      std: [63.02383 62.13708 66.74233]
      --------------------------------------------------------------------------------
      Creating a 'conv' model
      NOT IMPLEMENTED!!! Returning 0
      --------------------------------------------------------------------------------
      Starting session
      2018-02-28 23:13:35.927213: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
      2018-02-28 23:13:36.024251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      2018-02-28 23:13:36.024628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
      name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
      pciBusID: 0000:00:1e.0
      totalMemory: 11.17GiB freeMemory: 11.10GiB
      2018-02-28 23:13:36.024668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
      --------------------------------------------------------------------------------
      Starting training
      step=100    loss=2.22  val_acc=23 /100
      step=200    loss=2.11  val_acc=22 /100
      step=300    loss=2.01  val_acc=28 /100
      step=400    loss=1.84  val_acc=27 /100
      step=500    loss=1.72  val_acc=42 /100
      step=600    loss=1.75  val_acc=43 /100
      step=700    loss=1.57  val_acc=44 /100
      step=800    loss=1.70  val_acc=36 /100
      step=900    loss=1.41  val_acc=40 /100
      step=1000   loss=1.48  val_acc=42 /100
      step=1100   loss=1.51  val_acc=49 /100
      step=1200   loss=1.28  val_acc=60 /100
      step=1300   loss=1.18  val_acc=58 /100
      step=1400   loss=1.23  val_acc=59 /100
      step=1500   loss=1.16  val_acc=53 /100
      step=1600   loss=1.36  val_acc=38 /100
      step=1700   loss=1.29  val_acc=53 /100
      step=1800   loss=1.25  val_acc=62 /100
      step=1900   loss=1.25  val_acc=48 /100
      step=2000   loss=0.98  val_acc=63 /100
      step=2100   loss=1.23  val_acc=58 /100
      step=2200   loss=0.99  val_acc=67 /100
      step=2300   loss=1.17  val_acc=68 /100
      step=2400   loss=1.04  val_acc=73 /100
      step=2500   loss=0.93  val_acc=69 /100
      step=2600   loss=0.98  val_acc=74 /100
      step=2700   loss=0.94  val_acc=82 /100
      step=2800   loss=0.81  val_acc=66 /100
      step=2900   loss=1.12  val_acc=64 /100
      step=3000   loss=0.94  val_acc=73 /100
      step=3100   loss=0.90  val_acc=71 /100
      step=3200   loss=0.74  val_acc=75 /100
      step=3300   loss=0.84  val_acc=73 /100
      step=3400   loss=0.83  val_acc=85 /100
      step=3500   loss=0.90  val_acc=75 /100
      step=3600   loss=0.91  val_acc=70 /100
      step=3700   loss=0.65  val_acc=84 /100
      step=3800   loss=0.71  val_acc=68 /100
      step=3900   loss=0.77  val_acc=78 /100
      step=4000   loss=0.82  val_acc=77 /100
      step=4100   loss=0.66  val_acc=82 /100
      step=4200   loss=0.54  val_acc=78 /100
      step=4300   loss=0.73  val_acc=80 /100
      step=4400   loss=0.64  val_acc=72 /100
      step=4500   loss=0.66  val_acc=77 /100
      step=4600   loss=0.67  val_acc=79 /100
      step=4700   loss=0.49  val_acc=76 /100
      step=4800   loss=0.72  val_acc=78 /100
      step=4900   loss=0.62  val_acc=84 /100
      step=5000   loss=0.62  val_acc=81 /100
      step=5100   loss=0.64  val_acc=77 /100
      step=5200   loss=0.56  val_acc=82 /100
      step=5300   loss=0.58  val_acc=80 /100
      step=5400   loss=0.51  val_acc=84 /100
      step=5500   loss=0.59  val_acc=84 /100
      step=5600   loss=0.54  val_acc=85 /100
      step=5700   loss=0.70  val_acc=75 /100
      step=5800   loss=0.45  val_acc=83 /100
      step=5900   loss=0.42  val_acc=83 /100
      step=6000   loss=0.45  val_acc=79 /100
      step=6100   loss=0.59  val_acc=77 /100
      step=6200   loss=0.57  val_acc=84 /100
      step=6300   loss=0.45  val_acc=80 /100
      step=6400   loss=0.53  val_acc=88 /100
      step=6500   loss=0.42  val_acc=75 /100
      step=6600   loss=0.41  val_acc=79 /100
      step=6700   loss=0.47  val_acc=86 /100
      step=6800   loss=0.45  val_acc=84 /100
      step=6900   loss=0.56  val_acc=77 /100
      step=7000   loss=0.41  val_acc=84 /100
      step=7100   loss=0.45  val_acc=83 /100
      step=7200   loss=0.57  val_acc=85 /100
      step=7300   loss=0.46  val_acc=82 /100
      step=7400   loss=0.49  val_acc=84 /100
      step=7500   loss=0.48  val_acc=78 /100
      step=7600   loss=0.48  val_acc=86 /100
      step=7700   loss=0.27  val_acc=86 /100
      step=7800   loss=0.40  val_acc=81 /100
      step=7900   loss=0.43  val_acc=87 /100
      step=8000   loss=0.38  val_acc=84 /100
      step=8100   loss=0.29  val_acc=79 /100
      step=8200   loss=0.53  val_acc=90 /100
      step=8300   loss=0.42  val_acc=87 /100
      step=8400   loss=0.35  val_acc=91 /100
      step=8500   loss=0.34  val_acc=88 /100
      step=8600   loss=0.41  val_acc=78 /100
      step=8700   loss=0.36  val_acc=89 /100
      step=8800   loss=0.37  val_acc=78 /100
      step=8900   loss=0.23  val_acc=93 /100
      step=9000   loss=0.47  val_acc=83 /100
      step=9100   loss=0.19  val_acc=89 /100
      step=9200   loss=0.35  val_acc=87 /100
      step=9300   loss=0.27  val_acc=89 /100
      step=9400   loss=0.35  val_acc=83 /100
      step=9500   loss=0.40  val_acc=81 /100
      step=9600   loss=0.31  val_acc=83 /100
      step=9700   loss=0.24  val_acc=87 /100
      step=9800   loss=0.24  val_acc=83 /100
      step=9900   loss=0.20  val_acc=89 /100
      step=10000  loss=0.51  val_acc=85 /100
      step=10100  loss=0.33  val_acc=81 /100
      step=10200  loss=0.35  val_acc=89 /100
      step=10300  loss=0.22  val_acc=85 /100
      step=10400  loss=0.32  val_acc=83 /100
      step=10500  loss=0.27  val_acc=86 /100
      step=10600  loss=0.31  val_acc=86 /100
      step=10700  loss=0.30  val_acc=83 /100
      step=10800  loss=0.23  val_acc=87 /100
      step=10900  loss=0.28  val_acc=86 /100
      step=11000  loss=0.24  val_acc=81 /100
      step=11100  loss=0.20  val_acc=85 /100
      step=11200  loss=0.23  val_acc=86 /100
      step=11300  loss=0.25  val_acc=85 /100
      step=11400  loss=0.22  val_acc=82 /100
      step=11500  loss=0.17  val_acc=81 /100
      step=11600  loss=0.21  val_acc=83 /100
      step=11700  loss=0.20  val_acc=87 /100
      step=11800  loss=0.22  val_acc=85 /100
      step=11900  loss=0.20  val_acc=88 /100
      step=12000  loss=0.30  val_acc=85 /100
      step=12100  loss=0.25  val_acc=80 /100
      step=12200  loss=0.21  val_acc=90 /100
      step=12300  loss=0.37  val_acc=87 /100
      step=12400  loss=0.20  val_acc=89 /100
      step=12500  loss=0.18  val_acc=83 /100
      step=12600  loss=0.19  val_acc=87 /100
      step=12700  loss=0.15  val_acc=88 /100
      step=12800  loss=0.22  val_acc=83 /100
      step=12900  loss=0.23  val_acc=88 /100
      step=13000  loss=0.15  val_acc=85 /100
      step=13100  loss=0.26  val_acc=87 /100
      step=13200  loss=0.11  val_acc=88 /100
      step=13300  loss=0.21  val_acc=87 /100
      step=13400  loss=0.16  val_acc=86 /100
      step=13500  loss=0.15  val_acc=83 /100
      step=13600  loss=0.14  val_acc=85 /100
      step=13700  loss=0.14  val_acc=93 /100
      step=13800  loss=0.18  val_acc=81 /100
      step=13900  loss=0.17  val_acc=92 /100
      step=14000  loss=0.20  val_acc=90 /100
      step=14100  loss=0.11  val_acc=91 /100
      step=14200  loss=0.27  val_acc=85 /100
      step=14300  loss=0.26  val_acc=86 /100
      step=14400  loss=0.13  val_acc=83 /100
      step=14500  loss=0.19  val_acc=82 /100
      step=14600  loss=0.24  val_acc=89 /100
      step=14700  loss=0.15  val_acc=88 /100
      step=14800  loss=0.22  val_acc=91 /100
      step=14900  loss=0.14  val_acc=87 /100
      step=15000  loss=0.14  val_acc=88 /100
      step=15100  loss=0.30  val_acc=87 /100
      step=15200  loss=0.15  val_acc=86 /100
      step=15300  loss=0.14  val_acc=88 /100
      step=15400  loss=0.18  val_acc=92 /100
      step=15500  loss=0.16  val_acc=81 /100
      step=15600  loss=0.23  val_acc=89 /100
      step=15700  loss=0.11  val_acc=84 /100
      step=15800  loss=0.23  val_acc=85 /100
      step=15900  loss=0.12  val_acc=85 /100
      step=16000  loss=0.26  val_acc=79 /100
      step=16100  loss=0.13  val_acc=88 /100
      step=16200  loss=0.18  val_acc=86 /100
      step=16300  loss=0.12  val_acc=85 /100
      step=16400  loss=0.15  val_acc=83 /100
      step=16500  loss=0.16  val_acc=80 /100
      step=16600  loss=0.12  val_acc=88 /100
      step=16700  loss=0.11  val_acc=90 /100
      step=16800  loss=0.13  val_acc=90 /100
      step=16900  loss=0.13  val_acc=87 /100
      step=17000  loss=0.08  val_acc=93 /100
      step=17100  loss=0.08  val_acc=83 /100
      step=17200  loss=0.09  val_acc=90 /100
      step=17300  loss=0.08  val_acc=89 /100
      step=17400  loss=0.12  val_acc=90 /100
      step=17500  loss=0.17  val_acc=87 /100
      step=17600  loss=0.07  val_acc=89 /100
      step=17700  loss=0.17  val_acc=85 /100
      step=17800  loss=0.14  val_acc=82 /100
      step=17900  loss=0.10  val_acc=88 /100
      step=18000  loss=0.15  val_acc=87 /100
      step=18100  loss=0.07  val_acc=86 /100
      step=18200  loss=0.07  val_acc=89 /100
      step=18300  loss=0.05  val_acc=86 /100
      step=18400  loss=0.19  val_acc=85 /100
      step=18500  loss=0.15  val_acc=87 /100
      step=18600  loss=0.16  val_acc=82 /100
      step=18700  loss=0.06  val_acc=90 /100
      step=18800  loss=0.10  val_acc=79 /100
      step=18900  loss=0.08  val_acc=93 /100
      step=19000  loss=0.13  val_acc=89 /100
      step=19100  loss=0.08  val_acc=90 /100
      step=19200  loss=0.10  val_acc=82 /100
      step=19300  loss=0.06  val_acc=91 /100
      step=19400  loss=0.09  val_acc=81 /100
      step=19500  loss=0.06  val_acc=84 /100
      step=19600  loss=0.12  val_acc=87 /100
      step=19700  loss=0.12  val_acc=91 /100
      step=19800  loss=0.06  val_acc=85 /100
      step=19900  loss=0.09  val_acc=89 /100
      step=20000  loss=0.09  val_acc=85 /100
      --------------------------------------------------------------------------------
      Training done. Eval on TEST set
      test_accuracy:  8543/10000
